{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import scipy.io\n",
    "import h5py\n",
    "import torch.utils.data as Data\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'deepsea_train/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VALIDATION DATA ###\n",
    "valid_data_raw = scipy.io.loadmat(DATA_PATH+'valid.mat')\n",
    "x_valid = torch.FloatTensor(valid_data_raw['validxdata'])\n",
    "y_valid = torch.FloatTensor(valid_data_raw['validdata'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each training sample consists of a 1,000-bp sequence from the human GRCh37 reference genome centered on each 200-bp bin and is paired with a label vector for 919 chromatin features. \n",
    "\n",
    "The 1,000-bp DNA sequence is represented by a 1,000 Ã— 4 binary matrix, with columns corresponding to A, G, C and T."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Sea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeats = 4\n",
    "height = 1\n",
    "nkernels = [320,480,960]\n",
    "dropouts = [0.2,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deep_sea_nn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=nfeats,      out_channels=nkernels[0], kernel_size=8)\n",
    "        self.conv2 = nn.Conv1d(in_channels=nkernels[0], out_channels=nkernels[1], kernel_size=8)\n",
    "        self.conv3 = nn.Conv1d(in_channels=nkernels[1], out_channels=nkernels[2], kernel_size=8)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        self.drop1 = nn.Dropout(p=dropouts[0])\n",
    "        self.drop2 = nn.Dropout(p=dropouts[1])\n",
    "        self.linear1 = nn.Linear(53*960, 925)\n",
    "        self.linear2 = nn.Linear(925, 919)\n",
    "    \n",
    "    def foward(self, input):\n",
    "        ## convolution 1 ##\n",
    "        ds = self.conv1(input)\n",
    "        ds = F.relu(ds)\n",
    "        ds = self.maxpool(ds)\n",
    "        ds = self.drop1(ds)\n",
    "        \n",
    "        ## convolution 2 ##\n",
    "        ds = self.conv2(ds)\n",
    "        ds = F.relu(ds)\n",
    "        ds = self.maxpool(ds)\n",
    "        ds = self.drop1(ds)\n",
    "        \n",
    "        ## convolution 3 ##\n",
    "        ds = self.conv3(ds)\n",
    "        ds = F.relu(ds)\n",
    "        ds = self.drop2(ds)\n",
    "        \n",
    "        ds = ds.view(-1, 53*960)\n",
    "        ds = self.linear1(ds)\n",
    "        ds = F.relu(ds)\n",
    "        ds = self.linear2(ds)\n",
    "        \n",
    "        return ds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HYPERPARMETERS\n",
    "params = {'batch_size': 100,'num_workers': 2}\n",
    "device = 'cuda'\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep_sea_nn(\n",
      "  (conv1): Conv1d(4, 320, kernel_size=(8,), stride=(1,))\n",
      "  (conv2): Conv1d(320, 480, kernel_size=(8,), stride=(1,))\n",
      "  (conv3): Conv1d(480, 960, kernel_size=(8,), stride=(1,))\n",
      "  (maxpool): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (drop1): Dropout(p=0.2)\n",
      "  (drop2): Dropout(p=0.5)\n",
      "  (linear1): Linear(in_features=50880, out_features=925, bias=True)\n",
      "  (linear2): Linear(in_features=925, out_features=919, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "deep_sea = deep_sea_nn()\n",
    "deep_sea.to(device)\n",
    "print(deep_sea)\n",
    "\n",
    "optimizer = optim.SGD(deep_sea.parameters(), lr=learning_rate,momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5,verbose=1)\n",
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = Data.DataLoader(dataset=Data.TensorDataset(x_valid, y_valid), shuffle=False,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Training Epoch 1/10.. Training data Part: 0.. \n",
      "#### Training part time elapsed: 197.79401326179504\n",
      "#### Training Epoch 1/10.. Training data Part: 1.. \n",
      "#### Training part time elapsed: 196.35107254981995\n",
      "#### Training Epoch 1/10.. Training data Part: 2.. \n",
      "#### Training part time elapsed: 190.28271985054016\n",
      "#### Training Epoch 1/10.. Training data Part: 3.. \n",
      "#### Training part time elapsed: 190.31472516059875\n",
      "#### Training Epoch 1/10.. Training data Part: 4.. \n",
      "#### Training part time elapsed: 190.23392343521118\n",
      "#### Training Epoch 1/10.. Training data Part: 5.. \n",
      "#### Training part time elapsed: 190.38456225395203\n",
      "#### Training Epoch 1/10.. Training data Part: 6.. \n",
      "#### Training part time elapsed: 190.350346326828\n",
      "#### Training Epoch 1/10.. Training data Part: 7.. \n",
      "#### Training part time elapsed: 190.38146138191223\n",
      "#### Training Epoch 1/10.. Training data Part: 8.. \n",
      "#### Training part time elapsed: 191.61573696136475\n",
      "#### Training Epoch 1/10.. Training data Part: 9.. \n",
      "#### Training part time elapsed: 193.99666166305542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erick/anaconda3/envs/secure_private_ai/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type deep_sea_nn. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10.. Train loss: 0.09881587326526642.. Validation loss: 0.08267591148614883.. \n",
      "#### Training Epoch 2/10.. Training data Part: 0.. \n",
      "#### Training part time elapsed: 190.67163372039795\n",
      "#### Training Epoch 2/10.. Training data Part: 1.. \n",
      "#### Training part time elapsed: 194.55890226364136\n",
      "#### Training Epoch 2/10.. Training data Part: 2.. \n",
      "#### Training part time elapsed: 193.54051995277405\n",
      "#### Training Epoch 2/10.. Training data Part: 3.. \n",
      "#### Training part time elapsed: 192.52011895179749\n",
      "#### Training Epoch 2/10.. Training data Part: 4.. \n",
      "#### Training part time elapsed: 192.25975799560547\n",
      "#### Training Epoch 2/10.. Training data Part: 5.. \n",
      "#### Training part time elapsed: 192.20218181610107\n",
      "#### Training Epoch 2/10.. Training data Part: 6.. \n",
      "#### Training part time elapsed: 192.2119061946869\n",
      "#### Training Epoch 2/10.. Training data Part: 7.. \n",
      "#### Training part time elapsed: 192.29465055465698\n",
      "#### Training Epoch 2/10.. Training data Part: 8.. \n",
      "#### Training part time elapsed: 192.51175904273987\n",
      "#### Training Epoch 2/10.. Training data Part: 9.. \n",
      "#### Training part time elapsed: 192.23651361465454\n",
      "Epoch 2/10.. Train loss: 0.09172996878623962.. Validation loss: 0.07776300609111786.. \n",
      "#### Training Epoch 3/10.. Training data Part: 0.. \n",
      "#### Training part time elapsed: 192.22142219543457\n",
      "#### Training Epoch 3/10.. Training data Part: 1.. \n",
      "#### Training part time elapsed: 192.12876677513123\n",
      "#### Training Epoch 3/10.. Training data Part: 2.. \n",
      "#### Training part time elapsed: 192.46735310554504\n",
      "#### Training Epoch 3/10.. Training data Part: 3.. \n",
      "#### Training part time elapsed: 192.26014709472656\n",
      "#### Training Epoch 3/10.. Training data Part: 4.. \n",
      "#### Training part time elapsed: 192.3715798854828\n",
      "#### Training Epoch 3/10.. Training data Part: 5.. \n",
      "#### Training part time elapsed: 192.19221258163452\n",
      "#### Training Epoch 3/10.. Training data Part: 6.. \n",
      "#### Training part time elapsed: 192.10316252708435\n",
      "#### Training Epoch 3/10.. Training data Part: 7.. \n",
      "#### Training part time elapsed: 192.20942544937134\n",
      "#### Training Epoch 3/10.. Training data Part: 8.. \n",
      "#### Training part time elapsed: 192.1915318965912\n",
      "#### Training Epoch 3/10.. Training data Part: 9.. \n",
      "#### Training part time elapsed: 192.15466356277466\n",
      "Epoch 3/10.. Train loss: 0.07784777879714966.. Validation loss: 0.08223871886730194.. \n",
      "#### Training Epoch 4/10.. Training data Part: 0.. \n",
      "#### Training part time elapsed: 192.15466165542603\n",
      "#### Training Epoch 4/10.. Training data Part: 1.. \n",
      "#### Training part time elapsed: 192.2517695426941\n",
      "#### Training Epoch 4/10.. Training data Part: 2.. \n",
      "#### Training part time elapsed: 192.14616227149963\n",
      "#### Training Epoch 4/10.. Training data Part: 3.. \n",
      "#### Training part time elapsed: 192.14746594429016\n",
      "#### Training Epoch 4/10.. Training data Part: 4.. \n",
      "#### Training part time elapsed: 192.07438111305237\n",
      "#### Training Epoch 4/10.. Training data Part: 5.. \n",
      "#### Training part time elapsed: 192.1555004119873\n",
      "#### Training Epoch 4/10.. Training data Part: 6.. \n",
      "#### Training part time elapsed: 191.97067284584045\n",
      "#### Training Epoch 4/10.. Training data Part: 7.. \n",
      "#### Training part time elapsed: 192.15503931045532\n",
      "#### Training Epoch 4/10.. Training data Part: 8.. \n",
      "#### Training part time elapsed: 192.02776455879211\n",
      "#### Training Epoch 4/10.. Training data Part: 9.. \n",
      "#### Training part time elapsed: 192.15773606300354\n",
      "Epoch 4/10.. Train loss: 0.08226369321346283.. Validation loss: 0.07605240494012833.. \n",
      "#### Training Epoch 5/10.. Training data Part: 0.. \n",
      "#### Training part time elapsed: 191.9761188030243\n",
      "#### Training Epoch 5/10.. Training data Part: 1.. \n",
      "#### Training part time elapsed: 192.01857161521912\n",
      "#### Training Epoch 5/10.. Training data Part: 2.. \n",
      "#### Training part time elapsed: 192.0585799217224\n",
      "#### Training Epoch 5/10.. Training data Part: 3.. \n",
      "#### Training part time elapsed: 191.89867544174194\n",
      "#### Training Epoch 5/10.. Training data Part: 4.. \n",
      "#### Training part time elapsed: 191.942538022995\n",
      "#### Training Epoch 5/10.. Training data Part: 5.. \n",
      "#### Training part time elapsed: 192.10041451454163\n",
      "#### Training Epoch 5/10.. Training data Part: 6.. \n",
      "#### Training part time elapsed: 192.0036928653717\n",
      "#### Training Epoch 5/10.. Training data Part: 7.. \n",
      "#### Training part time elapsed: 192.09968447685242\n",
      "#### Training Epoch 5/10.. Training data Part: 8.. \n",
      "#### Training part time elapsed: 192.15493845939636\n",
      "#### Training Epoch 5/10.. Training data Part: 9.. \n",
      "#### Training part time elapsed: 192.14444136619568\n",
      "Epoch 5/10.. Train loss: 0.07899907976388931.. Validation loss: 0.07438723742961884.. \n",
      "#### Training Epoch 6/10.. Training data Part: 0.. \n",
      "#### Training part time elapsed: 191.9616551399231\n",
      "#### Training Epoch 6/10.. Training data Part: 1.. \n",
      "#### Training part time elapsed: 192.08887839317322\n",
      "#### Training Epoch 6/10.. Training data Part: 2.. \n",
      "#### Training part time elapsed: 192.05100631713867\n",
      "#### Training Epoch 6/10.. Training data Part: 3.. \n",
      "#### Training part time elapsed: 192.07227325439453\n",
      "#### Training Epoch 6/10.. Training data Part: 4.. \n",
      "#### Training part time elapsed: 192.07140445709229\n",
      "#### Training Epoch 6/10.. Training data Part: 5.. \n",
      "#### Training part time elapsed: 191.77483320236206\n",
      "#### Training Epoch 6/10.. Training data Part: 6.. \n",
      "#### Training part time elapsed: 191.85264611244202\n",
      "#### Training Epoch 6/10.. Training data Part: 7.. \n",
      "#### Training part time elapsed: 191.76798915863037\n",
      "#### Training Epoch 6/10.. Training data Part: 8.. \n",
      "#### Training part time elapsed: 191.8755578994751\n",
      "#### Training Epoch 6/10.. Training data Part: 9.. \n",
      "#### Training part time elapsed: 192.04096102714539\n",
      "Epoch 6/10.. Train loss: 0.08825893700122833.. Validation loss: 0.07735223323106766.. \n",
      "#### Training Epoch 7/10.. Training data Part: 0.. \n",
      "#### Training part time elapsed: 191.8072850704193\n",
      "#### Training Epoch 7/10.. Training data Part: 1.. \n",
      "#### Training part time elapsed: 191.91444373130798\n",
      "#### Training Epoch 7/10.. Training data Part: 2.. \n",
      "#### Training part time elapsed: 191.9058380126953\n",
      "#### Training Epoch 7/10.. Training data Part: 3.. \n",
      "#### Training part time elapsed: 191.87081146240234\n",
      "#### Training Epoch 7/10.. Training data Part: 4.. \n",
      "#### Training part time elapsed: 191.8152678012848\n",
      "#### Training Epoch 7/10.. Training data Part: 5.. \n",
      "#### Training part time elapsed: 191.82989120483398\n",
      "#### Training Epoch 7/10.. Training data Part: 6.. \n",
      "#### Training part time elapsed: 191.7288899421692\n",
      "#### Training Epoch 7/10.. Training data Part: 7.. \n",
      "#### Training part time elapsed: 191.64244079589844\n",
      "#### Training Epoch 7/10.. Training data Part: 8.. \n",
      "#### Training part time elapsed: 191.6372058391571\n",
      "#### Training Epoch 7/10.. Training data Part: 9.. \n",
      "#### Training part time elapsed: 191.8327760696411\n",
      "Epoch 7/10.. Train loss: 0.06671270728111267.. Validation loss: 0.07620855420827866.. \n",
      "#### Training Epoch 8/10.. Training data Part: 0.. \n",
      "#### Training part time elapsed: 191.86061072349548\n",
      "#### Training Epoch 8/10.. Training data Part: 1.. \n",
      "#### Training part time elapsed: 191.88991451263428\n",
      "#### Training Epoch 8/10.. Training data Part: 2.. \n",
      "#### Training part time elapsed: 191.85024189949036\n",
      "#### Training Epoch 8/10.. Training data Part: 3.. \n",
      "#### Training part time elapsed: 191.91357421875\n",
      "#### Training Epoch 8/10.. Training data Part: 4.. \n",
      "#### Training part time elapsed: 191.87000727653503\n",
      "#### Training Epoch 8/10.. Training data Part: 5.. \n",
      "#### Training part time elapsed: 191.74409246444702\n",
      "#### Training Epoch 8/10.. Training data Part: 6.. \n",
      "#### Training part time elapsed: 191.65058875083923\n",
      "#### Training Epoch 8/10.. Training data Part: 7.. \n",
      "#### Training part time elapsed: 191.71815180778503\n",
      "#### Training Epoch 8/10.. Training data Part: 8.. \n",
      "#### Training part time elapsed: 191.84518480300903\n",
      "#### Training Epoch 8/10.. Training data Part: 9.. \n",
      "#### Training part time elapsed: 191.77417850494385\n",
      "Epoch 8/10.. Train loss: 0.07520079612731934.. Validation loss: 0.07456742972135544.. \n",
      "#### Training Epoch 9/10.. Training data Part: 0.. \n",
      "#### Training part time elapsed: 191.63180804252625\n",
      "#### Training Epoch 9/10.. Training data Part: 1.. \n",
      "#### Training part time elapsed: 191.5762162208557\n",
      "#### Training Epoch 9/10.. Training data Part: 2.. \n",
      "#### Training part time elapsed: 191.7066309452057\n",
      "#### Training Epoch 9/10.. Training data Part: 3.. \n",
      "#### Training part time elapsed: 191.65180587768555\n",
      "#### Training Epoch 9/10.. Training data Part: 4.. \n",
      "#### Training part time elapsed: 191.5469467639923\n",
      "#### Training Epoch 9/10.. Training data Part: 5.. \n",
      "#### Training part time elapsed: 191.73459267616272\n",
      "#### Training Epoch 9/10.. Training data Part: 6.. \n",
      "#### Training part time elapsed: 191.5248076915741\n",
      "#### Training Epoch 9/10.. Training data Part: 7.. \n",
      "#### Training part time elapsed: 191.35201382637024\n",
      "#### Training Epoch 9/10.. Training data Part: 8.. \n",
      "#### Training part time elapsed: 191.594801902771\n",
      "#### Training Epoch 9/10.. Training data Part: 9.. \n",
      "#### Training part time elapsed: 191.46740794181824\n",
      "Epoch 9/10.. Train loss: 0.06984399259090424.. Validation loss: 0.07484239339828491.. \n",
      "#### Training Epoch 10/10.. Training data Part: 0.. \n",
      "#### Training part time elapsed: 191.48679852485657\n",
      "#### Training Epoch 10/10.. Training data Part: 1.. \n",
      "#### Training part time elapsed: 191.57743096351624\n",
      "#### Training Epoch 10/10.. Training data Part: 2.. \n",
      "#### Training part time elapsed: 191.42068719863892\n",
      "#### Training Epoch 10/10.. Training data Part: 3.. \n",
      "#### Training part time elapsed: 191.58145880699158\n",
      "#### Training Epoch 10/10.. Training data Part: 4.. \n",
      "#### Training part time elapsed: 191.51371264457703\n",
      "#### Training Epoch 10/10.. Training data Part: 5.. \n",
      "#### Training part time elapsed: 191.55516028404236\n",
      "#### Training Epoch 10/10.. Training data Part: 6.. \n",
      "#### Training part time elapsed: 191.56381106376648\n",
      "#### Training Epoch 10/10.. Training data Part: 7.. \n",
      "#### Training part time elapsed: 191.58256649971008\n",
      "#### Training Epoch 10/10.. Training data Part: 8.. \n",
      "#### Training part time elapsed: 191.4653844833374\n",
      "#### Training Epoch 10/10.. Training data Part: 9.. \n",
      "#### Training part time elapsed: 191.58881044387817\n",
      "Epoch 10/10.. Train loss: 0.05454450100660324.. Validation loss: 0.06732532382011414.. \n",
      "#### Training time : 21231.24547457695\n"
     ]
    }
   ],
   "source": [
    "train_losses, valid_losses = [], []\n",
    "running_loss = 0\n",
    "running_val_loss = 0\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    deep_sea.train()\n",
    "    ### for each part of the training set ###\n",
    "    for i in range(0,10):\n",
    "        x_train_part = torch.FloatTensor(np.load(DATA_PATH+\"x_train_part_{}.npy\".format(i)))\n",
    "        y_train_part = torch.FloatTensor(np.load(DATA_PATH+\"y_train_part_{}.npy\".format(i)))\n",
    "        \n",
    "        train_loader = Data.DataLoader(dataset=Data.TensorDataset(x_train_part, y_train_part), shuffle=True, **params)\n",
    "        print(f\"#### Training Epoch {epoch+1}/{epochs}.. \"\n",
    "              f\"Training data Part: {i}.. \")\n",
    "        \n",
    "        part_start_time = time.time()\n",
    "        for j, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "            \n",
    "            x, y = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = deep_sea.foward(x)\n",
    "            loss = loss_func(out.to(device), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss = loss.item()\n",
    "            train_losses.append(running_loss)\n",
    "            \n",
    "        part_time_elapsed = time.time() - part_start_time\n",
    "        print('#### Training part time elapsed:', part_time_elapsed)\n",
    "        \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        torch.save(deep_sea, 'deep_sea_model_training_epoch_{e}.pth'.format(e=epoch+1))\n",
    "        torch.save(deep_sea.state_dict(), 'deep_sea_model_training_epoch_{e}_params.pth'.format(e=epoch+1))\n",
    "            \n",
    "    ## Validation ##\n",
    "    running_val_loss = 0\n",
    "    for i, (inputs, labels) in enumerate(valid_loader):\n",
    "        deep_sea.eval()\n",
    "        with torch.no_grad():\n",
    "            x, y = inputs.to(device), labels.to(device)\n",
    "\n",
    "            val_out = deep_sea.foward(x)\n",
    "            val_loss = loss_func(val_out, y)\n",
    "\n",
    "            running_val_loss = val_loss.item()\n",
    "            valid_losses.append(running_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "          f\"Train loss: {running_loss}.. \"\n",
    "          f\"Validation loss: {running_val_loss}.. \")\n",
    "    \n",
    "            \n",
    "time_elapsed = time.time() - start_time\n",
    "print('#### Training time :', time_elapsed)\n",
    "\n",
    "torch.save(deep_sea, 'deep_sea_model.pth')\n",
    "torch.save(deep_sea.state_dict(), 'deep_sea_model_params.pth'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6937127709388733,\n",
       " 0.693519651889801,\n",
       " 0.693340539932251,\n",
       " 0.6929615139961243,\n",
       " 0.6925379037857056,\n",
       " 0.6920639276504517,\n",
       " 0.6914697289466858,\n",
       " 0.6908859610557556,\n",
       " 0.6899749636650085,\n",
       " 0.6893641948699951,\n",
       " 0.6885901689529419,\n",
       " 0.6876187920570374,\n",
       " 0.6866158246994019,\n",
       " 0.6857511401176453,\n",
       " 0.6845744252204895,\n",
       " 0.6833927631378174,\n",
       " 0.681948184967041,\n",
       " 0.6806046962738037,\n",
       " 0.6790158152580261,\n",
       " 0.6776013374328613,\n",
       " 0.6753840446472168,\n",
       " 0.6733049750328064,\n",
       " 0.6709102392196655,\n",
       " 0.6683917045593262,\n",
       " 0.6654162406921387,\n",
       " 0.6620590686798096,\n",
       " 0.6586476564407349,\n",
       " 0.6545103192329407,\n",
       " 0.650050699710846,\n",
       " 0.6437300443649292,\n",
       " 0.6404386162757874,\n",
       " 0.6317921876907349,\n",
       " 0.6224679946899414,\n",
       " 0.6139769554138184,\n",
       " 0.6031457185745239,\n",
       " 0.5918344259262085,\n",
       " 0.5766188502311707,\n",
       " 0.5625152587890625,\n",
       " 0.5442333817481995,\n",
       " 0.526820957660675,\n",
       " 0.503395140171051,\n",
       " 0.47789594531059265,\n",
       " 0.4517008066177368,\n",
       " 0.42092329263687134,\n",
       " 0.3891291916370392,\n",
       " 0.35286128520965576,\n",
       " 0.3164880871772766,\n",
       " 0.28481948375701904,\n",
       " 0.24950863420963287,\n",
       " 0.2298351228237152,\n",
       " 0.1898806393146515,\n",
       " 0.1348428726196289,\n",
       " 0.13388459384441376,\n",
       " 0.1678491085767746,\n",
       " 0.08738391846418381,\n",
       " 0.09750659763813019,\n",
       " 0.10410090535879135,\n",
       " 0.10544204711914062,\n",
       " 0.11614581942558289,\n",
       " 0.13363145291805267,\n",
       " 0.1317601203918457,\n",
       " 0.08581153303384781,\n",
       " 0.14076335728168488,\n",
       " 0.1066683828830719,\n",
       " 0.09219491481781006,\n",
       " 0.13224473595619202,\n",
       " 0.1709599494934082,\n",
       " 0.17044267058372498,\n",
       " 0.10940476506948471,\n",
       " 0.09944570809602737,\n",
       " 0.10185360908508301,\n",
       " 0.1834302395582199,\n",
       " 0.13477444648742676,\n",
       " 0.08225919306278229,\n",
       " 0.13816489279270172,\n",
       " 0.09315244853496552,\n",
       " 0.1028381809592247,\n",
       " 0.111376091837883,\n",
       " 0.10624442994594574,\n",
       " 0.08946223556995392,\n",
       " 0.10830464959144592,\n",
       " 0.09445157647132874,\n",
       " 0.09436267614364624,\n",
       " 0.0863369032740593,\n",
       " 0.089847631752491,\n",
       " 0.08116939663887024,\n",
       " 0.10447851568460464,\n",
       " 0.08043738454580307,\n",
       " 0.09255491942167282,\n",
       " 0.1039440780878067,\n",
       " 0.11928834021091461,\n",
       " 0.08415830880403519,\n",
       " 0.09065950661897659,\n",
       " 0.08450755476951599,\n",
       " 0.09851566702127457,\n",
       " 0.10478924959897995,\n",
       " 0.08301917463541031,\n",
       " 0.08010198920965195,\n",
       " 0.08604209870100021,\n",
       " 0.08252686262130737,\n",
       " 0.09716431051492691,\n",
       " 0.11136112362146378,\n",
       " 0.07305953651666641,\n",
       " 0.08040706813335419,\n",
       " 0.1036299467086792,\n",
       " 0.13573117554187775,\n",
       " 0.09831210225820541,\n",
       " 0.09182865917682648,\n",
       " 0.12638308107852936,\n",
       " 0.11070352047681808,\n",
       " 0.08054141700267792,\n",
       " 0.11036093533039093,\n",
       " 0.1105077788233757,\n",
       " 0.07682842016220093,\n",
       " 0.07984741777181625,\n",
       " 0.07376275211572647,\n",
       " 0.09730801731348038,\n",
       " 0.07958661019802094,\n",
       " 0.0978546217083931,\n",
       " 0.10546894371509552,\n",
       " 0.07217489928007126,\n",
       " 0.11372462660074234,\n",
       " 0.07951910048723221,\n",
       " 0.08293595910072327,\n",
       " 0.10977799445390701,\n",
       " 0.08390355855226517,\n",
       " 0.12827084958553314,\n",
       " 0.09299775958061218,\n",
       " 0.09723515063524246,\n",
       " 0.12049597501754761,\n",
       " 0.10934910178184509,\n",
       " 0.12292145192623138,\n",
       " 0.10117944329977036,\n",
       " 0.08826135843992233,\n",
       " 0.07716336846351624,\n",
       " 0.09142456948757172,\n",
       " 0.10098264366388321,\n",
       " 0.1096416786313057,\n",
       " 0.11456532031297684,\n",
       " 0.08720506727695465,\n",
       " 0.10239052772521973,\n",
       " 0.10939723253250122,\n",
       " 0.08490648865699768,\n",
       " 0.10719247162342072,\n",
       " 0.07395467162132263,\n",
       " 0.07596810907125473,\n",
       " 0.10903565585613251,\n",
       " 0.09911908954381943,\n",
       " 0.0755143091082573,\n",
       " 0.1010967493057251,\n",
       " 0.12344926595687866,\n",
       " 0.07978294789791107,\n",
       " 0.08051122725009918,\n",
       " 0.08521497994661331,\n",
       " 0.10914037376642227,\n",
       " 0.10952234268188477,\n",
       " 0.08457236737012863,\n",
       " 0.09964153170585632,\n",
       " 0.07109717279672623,\n",
       " 0.08393432199954987,\n",
       " 0.10141823440790176,\n",
       " 0.11424288153648376,\n",
       " 0.07935530692338943,\n",
       " 0.08133859187364578,\n",
       " 0.0846731886267662,\n",
       " 0.09567326307296753,\n",
       " 0.0849800854921341,\n",
       " 0.08894338458776474,\n",
       " 0.08334952592849731,\n",
       " 0.09746535122394562,\n",
       " 0.09779620170593262,\n",
       " 0.07071080803871155,\n",
       " 0.09732898324728012,\n",
       " 0.10893084853887558,\n",
       " 0.08460021764039993,\n",
       " 0.08171574026346207,\n",
       " 0.08975903689861298,\n",
       " 0.08656184375286102,\n",
       " 0.10981030017137527,\n",
       " 0.0869869589805603,\n",
       " 0.10439705848693848,\n",
       " 0.07058020681142807,\n",
       " 0.10528690367937088,\n",
       " 0.09472890198230743,\n",
       " 0.09193351864814758,\n",
       " 0.08850160986185074,\n",
       " 0.09731816500425339,\n",
       " 0.06335936486721039,\n",
       " 0.10343966633081436,\n",
       " 0.07519134879112244,\n",
       " 0.07999224215745926,\n",
       " 0.10120156407356262,\n",
       " 0.0775529071688652,\n",
       " 0.07528072595596313,\n",
       " 0.09533308446407318,\n",
       " 0.09092462062835693,\n",
       " 0.12494202703237534,\n",
       " 0.09152066707611084,\n",
       " 0.08149346709251404,\n",
       " 0.08952760696411133,\n",
       " 0.0949893519282341,\n",
       " 0.1145540326833725,\n",
       " 0.09474252164363861,\n",
       " 0.09068189561367035,\n",
       " 0.0907571092247963,\n",
       " 0.11165012419223785,\n",
       " 0.08241517096757889,\n",
       " 0.09667293727397919,\n",
       " 0.06987433135509491,\n",
       " 0.09948641806840897,\n",
       " 0.10122115910053253,\n",
       " 0.08419530838727951,\n",
       " 0.10145733505487442,\n",
       " 0.08878164738416672,\n",
       " 0.08215877413749695,\n",
       " 0.11726324260234833,\n",
       " 0.09726773202419281,\n",
       " 0.09945214539766312,\n",
       " 0.1192723959684372,\n",
       " 0.09015487879514694,\n",
       " 0.07945415377616882,\n",
       " 0.09225355088710785,\n",
       " 0.10325594991445541,\n",
       " 0.0925402119755745,\n",
       " 0.07998424768447876,\n",
       " 0.08180525153875351,\n",
       " 0.10431762039661407,\n",
       " 0.10212815552949905,\n",
       " 0.08059646934270859,\n",
       " 0.0913703516125679,\n",
       " 0.12840908765792847,\n",
       " 0.1006549745798111,\n",
       " 0.09546677768230438,\n",
       " 0.09118872880935669,\n",
       " 0.08658536523580551,\n",
       " 0.09490204602479935,\n",
       " 0.09053982049226761,\n",
       " 0.07265488803386688,\n",
       " 0.10500794649124146,\n",
       " 0.10532727092504501,\n",
       " 0.11797205358743668,\n",
       " 0.07483493536710739,\n",
       " 0.09200013428926468,\n",
       " 0.09978487342596054,\n",
       " 0.10814449936151505,\n",
       " 0.07769060879945755,\n",
       " 0.08943215757608414,\n",
       " 0.08803857117891312,\n",
       " 0.0888013243675232,\n",
       " 0.08526607602834702,\n",
       " 0.08485463261604309,\n",
       " 0.09351693093776703,\n",
       " 0.0940939337015152,\n",
       " 0.09155198931694031,\n",
       " 0.06969265639781952,\n",
       " 0.08111870288848877,\n",
       " 0.07951611280441284,\n",
       " 0.1030597984790802,\n",
       " 0.10185997188091278,\n",
       " 0.10470405966043472,\n",
       " 0.0904293805360794,\n",
       " 0.07906334102153778,\n",
       " 0.09867986291646957,\n",
       " 0.10964835435152054,\n",
       " 0.09840260446071625,\n",
       " 0.0854363813996315,\n",
       " 0.07994402945041656,\n",
       " 0.09687945991754532,\n",
       " 0.07235480844974518,\n",
       " 0.10597851127386093,\n",
       " 0.08078006654977798,\n",
       " 0.08940478414297104,\n",
       " 0.08731027692556381,\n",
       " 0.09869614988565445,\n",
       " 0.11494119465351105,\n",
       " 0.09848687052726746,\n",
       " 0.10145260393619537,\n",
       " 0.07788711041212082,\n",
       " 0.10416725277900696,\n",
       " 0.0952560305595398,\n",
       " 0.13156044483184814,\n",
       " 0.12749208509922028,\n",
       " 0.09531376510858536,\n",
       " 0.08298543095588684,\n",
       " 0.09014453738927841,\n",
       " 0.09776264429092407,\n",
       " 0.09313228726387024,\n",
       " 0.08210927248001099,\n",
       " 0.08566455543041229,\n",
       " 0.07896288484334946,\n",
       " 0.07727926224470139,\n",
       " 0.12153171747922897,\n",
       " 0.09019666165113449,\n",
       " 0.07949583232402802,\n",
       " 0.10790470242500305,\n",
       " 0.07572149485349655,\n",
       " 0.10819774866104126,\n",
       " 0.11674241721630096,\n",
       " 0.10038263350725174,\n",
       " 0.09695115685462952,\n",
       " 0.08014886826276779,\n",
       " 0.1288362294435501,\n",
       " 0.07123275101184845,\n",
       " 0.09640196710824966,\n",
       " 0.07430253177881241,\n",
       " 0.08632064610719681,\n",
       " 0.07694011926651001,\n",
       " 0.08988770842552185,\n",
       " 0.08457551151514053,\n",
       " 0.07996030896902084,\n",
       " 0.10471665114164352,\n",
       " 0.08767691999673843,\n",
       " 0.09617561846971512,\n",
       " 0.09145239740610123,\n",
       " 0.07801954448223114,\n",
       " 0.10936490446329117,\n",
       " 0.07885109633207321,\n",
       " 0.07288326323032379,\n",
       " 0.09291506558656693,\n",
       " 0.11507931351661682,\n",
       " 0.08723396062850952,\n",
       " 0.08880456537008286,\n",
       " 0.10269854217767715,\n",
       " 0.09385336190462112,\n",
       " 0.11704833060503006,\n",
       " 0.0702149048447609,\n",
       " 0.11184775829315186,\n",
       " 0.11374477297067642,\n",
       " 0.11744412779808044,\n",
       " 0.15074890851974487,\n",
       " 0.10208503901958466,\n",
       " 0.082504503428936,\n",
       " 0.10129133611917496,\n",
       " 0.09049531817436218,\n",
       " 0.1159827709197998,\n",
       " 0.09219411015510559,\n",
       " 0.106723353266716,\n",
       " 0.08043348044157028,\n",
       " 0.07530540972948074,\n",
       " 0.11375260353088379,\n",
       " 0.09216541796922684,\n",
       " 0.09037408232688904,\n",
       " 0.11332941055297852,\n",
       " 0.09613902866840363,\n",
       " 0.09676312655210495,\n",
       " 0.08206631988286972,\n",
       " 0.08968561142683029,\n",
       " 0.08056989312171936,\n",
       " 0.09658478200435638,\n",
       " 0.06686005741357803,\n",
       " 0.09382815659046173,\n",
       " 0.12470362335443497,\n",
       " 0.1066400408744812,\n",
       " 0.08675843477249146,\n",
       " 0.1027090772986412,\n",
       " 0.10787775367498398,\n",
       " 0.08165732771158218,\n",
       " 0.08035095781087875,\n",
       " 0.0882570743560791,\n",
       " 0.08482058346271515,\n",
       " 0.08808430284261703,\n",
       " 0.09741131216287613,\n",
       " 0.09559062123298645,\n",
       " 0.08367498964071274,\n",
       " 0.09721581637859344,\n",
       " 0.08646657317876816,\n",
       " 0.14548131823539734,\n",
       " 0.09206285327672958,\n",
       " 0.09490066766738892,\n",
       " 0.10300830006599426,\n",
       " 0.09161195904016495,\n",
       " 0.09699191153049469,\n",
       " 0.0924130529165268,\n",
       " 0.08649800717830658,\n",
       " 0.09323636442422867,\n",
       " 0.08992045372724533,\n",
       " 0.07978663593530655,\n",
       " 0.09353891760110855,\n",
       " 0.09257462620735168,\n",
       " 0.1101355105638504,\n",
       " 0.10868176072835922,\n",
       " 0.11367323249578476,\n",
       " 0.07023850083351135,\n",
       " 0.09507451951503754,\n",
       " 0.10223440825939178,\n",
       " 0.09719270467758179,\n",
       " 0.09743940830230713,\n",
       " 0.08886773884296417,\n",
       " 0.09330379962921143,\n",
       " 0.10243941098451614,\n",
       " 0.06304815411567688,\n",
       " 0.10645512491464615,\n",
       " 0.07916897535324097,\n",
       " 0.09205085784196854,\n",
       " 0.09256336838006973,\n",
       " 0.09385552257299423,\n",
       " 0.07542422413825989,\n",
       " 0.07723633199930191,\n",
       " 0.08935930579900742,\n",
       " 0.09484659135341644,\n",
       " 0.09468356519937515,\n",
       " 0.11804569512605667,\n",
       " 0.09371482580900192,\n",
       " 0.11456666141748428,\n",
       " 0.07545341551303864,\n",
       " 0.1044321283698082,\n",
       " 0.07832114398479462,\n",
       " 0.09538959711790085,\n",
       " 0.08592137694358826,\n",
       " 0.08748690783977509,\n",
       " 0.08870827406644821,\n",
       " 0.09132803231477737,\n",
       " 0.10926114767789841,\n",
       " 0.07034066319465637,\n",
       " 0.09728590399026871,\n",
       " 0.07548238337039948,\n",
       " 0.08195742964744568,\n",
       " 0.09224700927734375,\n",
       " 0.11429857462644577,\n",
       " 0.1087525486946106,\n",
       " 0.08207111060619354,\n",
       " 0.06795196235179901,\n",
       " 0.097368024289608,\n",
       " 0.10276978462934494,\n",
       " 0.11790713667869568,\n",
       " 0.08212453126907349,\n",
       " 0.06967099010944366,\n",
       " 0.07913507521152496,\n",
       " 0.08768721669912338,\n",
       " 0.09131547808647156,\n",
       " 0.0990009531378746,\n",
       " 0.08942478150129318,\n",
       " 0.0834524929523468,\n",
       " 0.0894973948597908,\n",
       " 0.0777803435921669,\n",
       " 0.12367593497037888,\n",
       " 0.06988806277513504,\n",
       " 0.08091649413108826,\n",
       " 0.08799894899129868,\n",
       " 0.080900639295578,\n",
       " 0.10241081565618515,\n",
       " 0.08724606037139893,\n",
       " 0.1215825080871582,\n",
       " 0.09258238971233368,\n",
       " 0.09140018373727798,\n",
       " 0.09686899930238724,\n",
       " 0.10125019401311874,\n",
       " 0.07418036460876465,\n",
       " 0.09366630017757416,\n",
       " 0.09703481197357178,\n",
       " 0.10838232934474945,\n",
       " 0.09698820114135742,\n",
       " 0.0829407274723053,\n",
       " 0.08240525424480438,\n",
       " 0.11525070667266846,\n",
       " 0.11952746659517288,\n",
       " 0.11233118921518326,\n",
       " 0.09692375361919403,\n",
       " 0.09948404133319855,\n",
       " 0.10263391584157944,\n",
       " 0.09974128007888794,\n",
       " 0.09846461564302444,\n",
       " 0.08258619904518127,\n",
       " 0.11290322989225388,\n",
       " 0.11413630843162537,\n",
       " 0.09998874366283417,\n",
       " 0.07670971006155014,\n",
       " 0.11839458346366882,\n",
       " 0.08245981484651566,\n",
       " 0.07610148936510086,\n",
       " 0.09560712426900864,\n",
       " 0.09637332707643509,\n",
       " 0.10525575280189514,\n",
       " 0.08996739983558655,\n",
       " 0.0730326771736145,\n",
       " 0.08293290436267853,\n",
       " 0.11915314197540283,\n",
       " 0.09867684543132782,\n",
       " 0.09610175341367722,\n",
       " 0.11629437655210495,\n",
       " 0.10477644205093384,\n",
       " 0.11180165410041809,\n",
       " 0.07728959619998932,\n",
       " 0.07403548806905746,\n",
       " 0.09234795719385147,\n",
       " 0.07949624955654144,\n",
       " 0.09185747802257538,\n",
       " 0.0738818421959877,\n",
       " 0.06700479984283447,\n",
       " 0.09278623014688492,\n",
       " 0.09764792770147324,\n",
       " 0.09928787499666214,\n",
       " 0.12366318702697754,\n",
       " 0.08975255489349365,\n",
       " 0.10309367626905441,\n",
       " 0.080022893846035,\n",
       " 0.07098255306482315,\n",
       " 0.08558013290166855,\n",
       " 0.09643843024969101,\n",
       " 0.10204695910215378,\n",
       " 0.06748153269290924,\n",
       " 0.08290325105190277,\n",
       " 0.09677359461784363,\n",
       " 0.08176444470882416,\n",
       " 0.07315203547477722,\n",
       " 0.09722056984901428,\n",
       " 0.10026954859495163,\n",
       " 0.08964735269546509,\n",
       " 0.07755856215953827,\n",
       " 0.07159966975450516,\n",
       " 0.0936586782336235,\n",
       " 0.09208765625953674,\n",
       " 0.10886969417333603,\n",
       " 0.07249241322278976,\n",
       " 0.1055559292435646,\n",
       " 0.09645894914865494,\n",
       " 0.09465537965297699,\n",
       " 0.07430963963270187,\n",
       " 0.0944172739982605,\n",
       " 0.1279943883419037,\n",
       " 0.10712485760450363,\n",
       " 0.12021388113498688,\n",
       " 0.1266733556985855,\n",
       " 0.10176388919353485,\n",
       " 0.09922848641872406,\n",
       " 0.1032702624797821,\n",
       " 0.09009906649589539,\n",
       " 0.09072567522525787,\n",
       " 0.09076880663633347,\n",
       " 0.08909950405359268,\n",
       " 0.08018938452005386,\n",
       " 0.09872220456600189,\n",
       " 0.09664430469274521,\n",
       " 0.09796029329299927,\n",
       " 0.10356266051530838,\n",
       " 0.07484442740678787,\n",
       " 0.07547780126333237,\n",
       " 0.09833213686943054,\n",
       " 0.11691012233495712,\n",
       " 0.07964975386857986,\n",
       " 0.07308956980705261,\n",
       " 0.09604591876268387,\n",
       " 0.09463898092508316,\n",
       " 0.07932521402835846,\n",
       " 0.11691837012767792,\n",
       " 0.0930299162864685,\n",
       " 0.10003600269556046,\n",
       " 0.07993243634700775,\n",
       " 0.0930577963590622,\n",
       " 0.10879088938236237,\n",
       " 0.10446209460496902,\n",
       " 0.07858750969171524,\n",
       " 0.07829855382442474,\n",
       " 0.07210102677345276,\n",
       " 0.10053451359272003,\n",
       " 0.10321757942438126,\n",
       " 0.10162970423698425,\n",
       " 0.10112664848566055,\n",
       " 0.11495470255613327,\n",
       " 0.07360917329788208,\n",
       " 0.07891149818897247,\n",
       " 0.0941629484295845,\n",
       " 0.09573358297348022,\n",
       " 0.08834925293922424,\n",
       " 0.05705851688981056,\n",
       " 0.08672802895307541,\n",
       " 0.11775914579629898,\n",
       " 0.10721128433942795,\n",
       " 0.10999795794487,\n",
       " 0.1032106950879097,\n",
       " 0.09669533371925354,\n",
       " 0.09425147622823715,\n",
       " 0.0970071479678154,\n",
       " 0.07839256525039673,\n",
       " 0.07918772846460342,\n",
       " 0.10995843261480331,\n",
       " 0.08443538099527359,\n",
       " 0.09758155792951584,\n",
       " 0.11307213455438614,\n",
       " 0.1224888414144516,\n",
       " 0.08448940515518188,\n",
       " 0.09141094982624054,\n",
       " 0.1359933316707611,\n",
       " 0.10871739685535431,\n",
       " 0.07699514925479889,\n",
       " 0.08123814314603806,\n",
       " 0.07755621522665024,\n",
       " 0.1292709857225418,\n",
       " 0.13181547820568085,\n",
       " 0.08858077228069305,\n",
       " 0.09473990648984909,\n",
       " 0.0794675201177597,\n",
       " 0.12025254964828491,\n",
       " 0.10205347836017609,\n",
       " 0.09741407632827759,\n",
       " 0.08342298120260239,\n",
       " 0.09285194426774979,\n",
       " 0.10301657766103745,\n",
       " 0.08816634118556976,\n",
       " 0.06899367272853851,\n",
       " 0.08543536067008972,\n",
       " 0.09557762742042542,\n",
       " 0.1104821190237999,\n",
       " 0.08221492916345596,\n",
       " 0.08815037459135056,\n",
       " 0.07621507346630096,\n",
       " 0.08279925584793091,\n",
       " 0.08231735229492188,\n",
       " 0.10029130429029465,\n",
       " 0.08182729035615921,\n",
       " 0.10542488843202591,\n",
       " 0.0907919630408287,\n",
       " 0.06312599778175354,\n",
       " 0.09695076942443848,\n",
       " 0.09119012206792831,\n",
       " 0.07692433893680573,\n",
       " 0.13262341916561127,\n",
       " 0.07949452847242355,\n",
       " 0.09683790057897568,\n",
       " 0.08929166942834854,\n",
       " 0.10961011797189713,\n",
       " 0.09014347940683365,\n",
       " 0.09557393193244934,\n",
       " 0.09500747919082642,\n",
       " 0.10143829882144928,\n",
       " 0.0765284076333046,\n",
       " 0.091359943151474,\n",
       " 0.10077912360429764,\n",
       " 0.09205818921327591,\n",
       " 0.08546819537878036,\n",
       " 0.10208780318498611,\n",
       " 0.07028073817491531,\n",
       " 0.10490956157445908,\n",
       " 0.10316727310419083,\n",
       " 0.07729118317365646,\n",
       " 0.0841657817363739,\n",
       " 0.10359149426221848,\n",
       " 0.09577883034944534,\n",
       " 0.09510477632284164,\n",
       " 0.09999022632837296,\n",
       " 0.09797482192516327,\n",
       " 0.07297466695308685,\n",
       " 0.09144014120101929,\n",
       " 0.09890463203191757,\n",
       " 0.09813235700130463,\n",
       " 0.07306291908025742,\n",
       " 0.0879257321357727,\n",
       " 0.08562611043453217,\n",
       " 0.08184551447629929,\n",
       " 0.09882084280252457,\n",
       " 0.07407702505588531,\n",
       " 0.10904441773891449,\n",
       " 0.08983425050973892,\n",
       " 0.08470173925161362,\n",
       " 0.1102057471871376,\n",
       " 0.08308660984039307,\n",
       " 0.07695095241069794,\n",
       " 0.10744219273328781,\n",
       " 0.10873591899871826,\n",
       " 0.08585759252309799,\n",
       " 0.10005364567041397,\n",
       " 0.0828624740242958,\n",
       " 0.10494191944599152,\n",
       " 0.11157545447349548,\n",
       " 0.09499672800302505,\n",
       " 0.09396030753850937,\n",
       " 0.10638430714607239,\n",
       " 0.08255580812692642,\n",
       " 0.0920756459236145,\n",
       " 0.0806160420179367,\n",
       " 0.09129169583320618,\n",
       " 0.07302512228488922,\n",
       " 0.08384975045919418,\n",
       " 0.10552458465099335,\n",
       " 0.08405574411153793,\n",
       " 0.11123007535934448,\n",
       " 0.09209135174751282,\n",
       " 0.09227734804153442,\n",
       " 0.09752421826124191,\n",
       " 0.08002432435750961,\n",
       " 0.11808209866285324,\n",
       " 0.1275203973054886,\n",
       " 0.11276945471763611,\n",
       " 0.07305284589529037,\n",
       " 0.0822695642709732,\n",
       " 0.08530013263225555,\n",
       " 0.11098287254571915,\n",
       " 0.1196795105934143,\n",
       " 0.09833880513906479,\n",
       " 0.08078459650278091,\n",
       " 0.08924555033445358,\n",
       " 0.13187755644321442,\n",
       " 0.10019492357969284,\n",
       " 0.09430808573961258,\n",
       " 0.08416333794593811,\n",
       " 0.08693931996822357,\n",
       " 0.11007372289896011,\n",
       " 0.11144953221082687,\n",
       " 0.08171109110116959,\n",
       " 0.11360812932252884,\n",
       " 0.09458144009113312,\n",
       " 0.12270236760377884,\n",
       " 0.09665722399950027,\n",
       " 0.09810905903577805,\n",
       " 0.09361577033996582,\n",
       " 0.09234008193016052,\n",
       " 0.08901342749595642,\n",
       " 0.0839783102273941,\n",
       " 0.08822258561849594,\n",
       " 0.08621824532747269,\n",
       " 0.07197082042694092,\n",
       " 0.1086060106754303,\n",
       " 0.10666048526763916,\n",
       " 0.07802243530750275,\n",
       " 0.07222406566143036,\n",
       " 0.09202994406223297,\n",
       " 0.08235908299684525,\n",
       " 0.07381036877632141,\n",
       " 0.09667150676250458,\n",
       " 0.13454504311084747,\n",
       " 0.08737096190452576,\n",
       " 0.07953812927007675,\n",
       " 0.09801342338323593,\n",
       " 0.09969663619995117,\n",
       " 0.09851767867803574,\n",
       " 0.08926500380039215,\n",
       " 0.08677060157060623,\n",
       " 0.10687181353569031,\n",
       " 0.07994082570075989,\n",
       " 0.1194232851266861,\n",
       " 0.07388748228549957,\n",
       " 0.10367228090763092,\n",
       " 0.10504020750522614,\n",
       " 0.09027575701475143,\n",
       " 0.09016000479459763,\n",
       " 0.08243796229362488,\n",
       " 0.07716554403305054,\n",
       " 0.10465351492166519,\n",
       " 0.09364557266235352,\n",
       " 0.1191396489739418,\n",
       " 0.128184512257576,\n",
       " 0.1169881820678711,\n",
       " 0.07483600080013275,\n",
       " 0.09359990060329437,\n",
       " 0.07167668640613556,\n",
       " 0.0943930447101593,\n",
       " 0.09887707978487015,\n",
       " 0.09890245646238327,\n",
       " 0.09684781730175018,\n",
       " 0.10975413024425507,\n",
       " 0.09577365964651108,\n",
       " 0.08763154596090317,\n",
       " 0.11546093225479126,\n",
       " 0.09586139768362045,\n",
       " 0.09473157674074173,\n",
       " 0.10619094222784042,\n",
       " 0.08826501667499542,\n",
       " 0.08101551979780197,\n",
       " 0.09272726625204086,\n",
       " 0.08154413849115372,\n",
       " 0.07660770416259766,\n",
       " 0.08897317945957184,\n",
       " 0.08895622193813324,\n",
       " 0.09106791019439697,\n",
       " 0.10110730677843094,\n",
       " 0.09104401618242264,\n",
       " 0.08059263974428177,\n",
       " 0.09722209721803665,\n",
       " 0.072838194668293,\n",
       " 0.09449756145477295,\n",
       " 0.0640002116560936,\n",
       " 0.09907151013612747,\n",
       " 0.09814763814210892,\n",
       " 0.0975704938173294,\n",
       " 0.0942966639995575,\n",
       " 0.08616353571414948,\n",
       " 0.12042288482189178,\n",
       " 0.08695951104164124,\n",
       " 0.10546335577964783,\n",
       " 0.09359488636255264,\n",
       " 0.08088529109954834,\n",
       " 0.09647448360919952,\n",
       " 0.10751774907112122,\n",
       " 0.08985888957977295,\n",
       " 0.0966237485408783,\n",
       " 0.10322775691747665,\n",
       " 0.07804158329963684,\n",
       " 0.08426934480667114,\n",
       " 0.1091223806142807,\n",
       " 0.08485567569732666,\n",
       " 0.07669513672590256,\n",
       " 0.09423692524433136,\n",
       " 0.10470090806484222,\n",
       " 0.09905911982059479,\n",
       " 0.08763710409402847,\n",
       " 0.08302623778581619,\n",
       " 0.08343805372714996,\n",
       " 0.10541003197431564,\n",
       " 0.1113211140036583,\n",
       " 0.1004096120595932,\n",
       " 0.08560705184936523,\n",
       " 0.08895189315080643,\n",
       " 0.1001155748963356,\n",
       " 0.08159371465444565,\n",
       " 0.09091359376907349,\n",
       " 0.08900448679924011,\n",
       " 0.12281879037618637,\n",
       " 0.10729837417602539,\n",
       " 0.09782513231039047,\n",
       " 0.0860217809677124,\n",
       " 0.10157986730337143,\n",
       " 0.089144766330719,\n",
       " 0.14075681567192078,\n",
       " 0.10226263105869293,\n",
       " 0.0880342423915863,\n",
       " 0.09708652645349503,\n",
       " 0.10363541543483734,\n",
       " 0.09923061728477478,\n",
       " 0.12158860266208649,\n",
       " 0.08924223482608795,\n",
       " 0.10028223693370819,\n",
       " 0.09109170734882355,\n",
       " 0.08259297162294388,\n",
       " 0.07566630840301514,\n",
       " 0.08569051325321198,\n",
       " 0.06987167149782181,\n",
       " 0.11621782183647156,\n",
       " 0.1005123183131218,\n",
       " 0.09508558362722397,\n",
       " 0.08568151295185089,\n",
       " 0.12649959325790405,\n",
       " 0.08727430552244186,\n",
       " 0.09609392285346985,\n",
       " 0.11254910379648209,\n",
       " 0.08981861174106598,\n",
       " 0.10717978328466415,\n",
       " 0.1001063883304596,\n",
       " 0.0977383628487587,\n",
       " 0.0758751630783081,\n",
       " 0.07844918966293335,\n",
       " 0.0945785790681839,\n",
       " 0.07861727476119995,\n",
       " 0.09649179130792618,\n",
       " 0.0831504538655281,\n",
       " 0.09065345674753189,\n",
       " 0.13078342378139496,\n",
       " 0.06905536353588104,\n",
       " 0.08738407492637634,\n",
       " 0.08455988764762878,\n",
       " 0.09788212180137634,\n",
       " 0.08583232015371323,\n",
       " 0.09628044813871384,\n",
       " 0.09943278878927231,\n",
       " 0.11123369634151459,\n",
       " 0.09107354283332825,\n",
       " 0.09845985472202301,\n",
       " 0.0740492194890976,\n",
       " 0.09734635055065155,\n",
       " 0.09629670530557632,\n",
       " 0.08755259215831757,\n",
       " 0.07700207084417343,\n",
       " 0.09833963215351105,\n",
       " 0.0795811340212822,\n",
       " 0.1138591542840004,\n",
       " 0.10096043348312378,\n",
       " 0.07882042974233627,\n",
       " 0.10328098386526108,\n",
       " 0.1262209713459015,\n",
       " 0.09560742229223251,\n",
       " 0.10013002157211304,\n",
       " 0.09382287412881851,\n",
       " 0.09896358847618103,\n",
       " 0.12092594057321548,\n",
       " 0.08820343017578125,\n",
       " 0.08339989185333252,\n",
       " 0.10475903749465942,\n",
       " 0.09572584182024002,\n",
       " 0.11723644286394119,\n",
       " 0.08648326992988586,\n",
       " 0.0838107168674469,\n",
       " 0.09195106476545334,\n",
       " 0.09830009937286377,\n",
       " 0.10518831014633179,\n",
       " 0.08990297466516495,\n",
       " 0.1179334744811058,\n",
       " 0.08340522646903992,\n",
       " 0.08135603368282318,\n",
       " 0.06338868290185928,\n",
       " 0.07615683972835541,\n",
       " 0.09373410046100616,\n",
       " 0.10792326182126999,\n",
       " 0.08948537707328796,\n",
       " 0.09378857165575027,\n",
       " 0.08742129057645798,\n",
       " 0.0821952149271965,\n",
       " 0.0993613749742508,\n",
       " 0.08890548348426819,\n",
       " 0.09167040139436722,\n",
       " 0.09609111398458481,\n",
       " 0.07627134025096893,\n",
       " 0.10165785998106003,\n",
       " 0.09924700856208801,\n",
       " 0.11770107597112656,\n",
       " 0.13284453749656677,\n",
       " 0.10060033947229385,\n",
       " 0.11912346631288528,\n",
       " 0.12659616768360138,\n",
       " 0.10948190093040466,\n",
       " 0.07265213131904602,\n",
       " 0.0929153636097908,\n",
       " 0.10116244107484818,\n",
       " 0.07304336130619049,\n",
       " 0.11862833052873611,\n",
       " 0.07878092676401138,\n",
       " 0.1120234951376915,\n",
       " 0.07526927441358566,\n",
       " 0.08791116625070572,\n",
       " 0.12390656024217606,\n",
       " 0.10895377397537231,\n",
       " 0.09288238734006882,\n",
       " 0.08227493613958359,\n",
       " 0.08351777493953705,\n",
       " 0.08055232465267181,\n",
       " 0.11124290525913239,\n",
       " 0.09007897228002548,\n",
       " 0.09691238403320312,\n",
       " 0.11217783391475677,\n",
       " 0.09284000098705292,\n",
       " 0.06634168326854706,\n",
       " 0.09470561146736145,\n",
       " 0.0940050333738327,\n",
       " 0.114546999335289,\n",
       " 0.09446021169424057,\n",
       " 0.09380827844142914,\n",
       " 0.10681066662073135,\n",
       " 0.09269186854362488,\n",
       " 0.09223870187997818,\n",
       " 0.09027349948883057,\n",
       " 0.09570454806089401,\n",
       " 0.06419191509485245,\n",
       " 0.08476009219884872,\n",
       " 0.08975290507078171,\n",
       " 0.080330990254879,\n",
       " 0.09858865290880203,\n",
       " 0.13737286627292633,\n",
       " 0.08668261766433716,\n",
       " 0.12307533621788025,\n",
       " 0.10242238640785217,\n",
       " 0.08736256510019302,\n",
       " 0.07264590263366699,\n",
       " 0.08875691890716553,\n",
       " 0.07971522212028503,\n",
       " 0.0802438035607338,\n",
       " 0.11410880833864212,\n",
       " 0.07346528768539429,\n",
       " 0.08787140250205994,\n",
       " 0.07574347406625748,\n",
       " 0.09228427708148956,\n",
       " 0.08664367347955704,\n",
       " 0.10120973736047745,\n",
       " 0.0959801971912384,\n",
       " 0.09293291717767715,\n",
       " 0.07215651124715805,\n",
       " 0.08046521991491318,\n",
       " 0.07240545749664307,\n",
       " 0.06372898072004318,\n",
       " 0.08852241933345795,\n",
       " 0.10931982845067978,\n",
       " 0.07692451030015945,\n",
       " 0.08394080400466919,\n",
       " 0.09642866253852844,\n",
       " 0.11388932168483734,\n",
       " 0.10087565332651138,\n",
       " 0.09369213879108429,\n",
       " 0.09708330035209656,\n",
       " 0.08015145361423492,\n",
       " 0.09851668775081635,\n",
       " 0.0982794463634491,\n",
       " 0.09082594513893127,\n",
       " 0.07444293797016144,\n",
       " 0.0950450450181961,\n",
       " 0.0945545881986618,\n",
       " 0.08777038007974625,\n",
       " 0.08763935416936874,\n",
       " 0.06865308433771133,\n",
       " 0.08223351091146469,\n",
       " 0.12011004984378815,\n",
       " 0.10082821547985077,\n",
       " 0.07720782607793808,\n",
       " 0.0865945965051651,\n",
       " 0.07730987668037415,\n",
       " 0.08971279114484787,\n",
       " 0.09350644797086716,\n",
       " 0.07671774923801422,\n",
       " 0.10263542085886002,\n",
       " 0.07926427572965622,\n",
       " 0.07639337331056595,\n",
       " 0.08314544707536697,\n",
       " 0.09554426372051239,\n",
       " 0.11103548854589462,\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses\n",
    "#valid_losse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
